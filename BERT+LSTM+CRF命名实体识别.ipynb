{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658dc8a3",
   "metadata": {},
   "source": [
    "讲一下\n",
    "- 模型保存部分有一行代码：`torch.save(model.state_dict(), 'sample_data/bert_lstm_crf.pth')`，它保存的是模型在训练过程中学到的全部参数（weights & biases）。它不包含模型的结构定义，只保存了“你已经训练出来的参数值”。\n",
    "日后可以直接用；但你需要先重新定义模型结构（跟你保存时用的结构一模一样），然后再把参数加载进去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5256eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "import json\n",
    "import os\n",
    "\n",
    "train_data = []\n",
    "dev_data = []\n",
    "\n",
    "for line in open('train.json','r',encoding='UTF-8'):\n",
    "    train_data.append(json.loads(line))\n",
    "\n",
    "for line in open('dev.json','r',encoding='UTF-8'):\n",
    "    dev_data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe11d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要构建标签\n",
    "import re\n",
    "\n",
    "label_type = {'o':0,'address':1,'company':2,'name':3,'organization':4,'pad': 5}\n",
    "\n",
    "def decode_label(d):\n",
    "#解析标签，以列表形式构成\n",
    "  text_len = len(d['text'])\n",
    "  label = [0]*text_len\n",
    "  types = d['label'].keys()\n",
    "  for t in types:\n",
    "    if t in label_type:\n",
    "      values = d['label'][t].values()\n",
    "      si = [v for value in values for v in value]\n",
    "      for i in si:\n",
    "        for j in range(i[0],i[1]+1):\n",
    "          label[j] = label_type[t]\n",
    "  return label\n",
    "\n",
    "\n",
    "\n",
    "def transfrom_data(data,mode):\n",
    "  data_texts = [d['text'] for d in data]\n",
    "  \n",
    "  if mode == 'train':\n",
    "    data_labels = []\n",
    "    for d in data:\n",
    "      data_labels.append(decode_label(d))\n",
    "    return (data_texts,data_labels)\n",
    "  \n",
    "  else:\n",
    "    return data_texts \n",
    "\n",
    "train_texts,train_labels = transfrom_data(train_data,'train')\n",
    "dev_texts,dev_labels = transfrom_data(dev_data,'train')\n",
    "test_texts = transfrom_data(train_data,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303eb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 使用bert的tokenizer将文字转化成数字。\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定为中文\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "clear_output()\n",
    "\n",
    "train_ids = []\n",
    "dev_ids = []\n",
    "\n",
    "tokens = [[tokenizer.tokenize(t)[0] for t in text] for text in train_texts]\n",
    "train_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "tokens = [[tokenizer.tokenize(t)[0] for t in text] for text in dev_texts]\n",
    "dev_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "dev_labels = [label for label in dev_labels]\n",
    "train_labels = [label for label in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04724447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\asus\\anaconda3\\envs\\uk1\\lib\\site-packages (from keras_preprocessing) (1.26.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\asus\\anaconda3\\envs\\uk1\\lib\\site-packages (from keras_preprocessing) (1.16.0)\n",
      "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Installing collected packages: keras_preprocessing\n",
      "Successfully installed keras_preprocessing-1.1.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f22087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader load ok\n",
      "dataloader load ok\n"
     ]
    }
   ],
   "source": [
    "! pip install keras_preprocessing\n",
    "import torch\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        self.label_type = {'o': 0, 'address': 1, 'company': 2, 'name': 3, 'organization': 4, 'pad': 5}\n",
    "        clear_output()\n",
    "\n",
    "    def pad(self,ids,labels):\n",
    "\n",
    "        input_ids = pad_sequences(ids,maxlen=60,dtype='long', value=0.0,truncating=\"post\", padding=\"post\")\n",
    "        tags = pad_sequences(labels,maxlen=60, value=self.label_type[\"pad\"], padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
    "        attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "        return (input_ids,tags,attention_masks)\n",
    "\n",
    "    def loader(self,ids,labels):\n",
    "        input_ids,tags,attention_masks = self.pad(ids,labels)\n",
    "        dataset = TensorDataset(torch.tensor(input_ids),torch.tensor(tags),torch.tensor(attention_masks))\n",
    "        dataloader = DataLoader(dataset,batch_size=64)\n",
    "        print('dataloader load ok')\n",
    "        return dataloader\n",
    "\n",
    "dataloaders = Dataset()\n",
    "trainloader = dataloaders.loader(train_ids,train_labels)\n",
    "devloader = dataloaders.loader(dev_ids,dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "885e2fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-crf\n",
      "  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip install pytorch-crf\n",
    "from transformers import BertPreTrainedModel,BertModel\n",
    "from torchcrf import CRF\n",
    "import torch.nn as nn\n",
    "class BertLstmCrf(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config,need_bilstm = False,rnn_dim = 128):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_dim = config.hidden_size\n",
    "        self.need_bilstm = need_bilstm\n",
    "        if need_bilstm:\n",
    "            self.bilstm = nn.LSTM(config.hidden_size, rnn_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "            self.out_dim = 2*rnn_dim\n",
    "        self.liner = nn.Linear(self.out_dim, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels,batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None,labels=None,):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        if self.need_bilstm:\n",
    "            sequence_output,_ = self.bilstm(sequence_output)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        sequence_output = self.liner(sequence_output)\n",
    "        loss = -1 * self.crf(sequence_output, labels, mask=attention_mask.byte()) if labels != None else None\n",
    "        output = self.crf.decode(sequence_output, attention_mask.byte())\n",
    "        \n",
    "        return [loss,output] if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\asus\\anaconda3\\envs\\uk1\\lib\\site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\asus\\anaconda3\\envs\\uk1\\lib\\site-packages (from seqeval) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\anaconda3\\envs\\uk1\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asus\\anaconda3\\envs\\uk1\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\anaconda3\\envs\\uk1\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16183 sha256=d739db285a0a670c049e8ea560ebb10a0fb456caa5da9d0a479ba51b039a71ae\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\e2\\a5\\92\\2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e85fd7d977048b8bef780a21fbea1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\UK1\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertLstmCrf were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'liner.bias', 'liner.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "! pip install seqeval\n",
    "from transformers import BertForTokenClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from seqeval.metrics import f1_score as f1\n",
    "import os\n",
    "\n",
    "model = BertLstmCrf.from_pretrained(\"bert-base-chinese\", num_labels=6)\n",
    "need_CRF = True\n",
    "# model =  BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_labels=6)\n",
    "# need_CRF = False\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "Epochs = 10\n",
    "type_label = {0:'o',1:'address',2:'company',3:'name',4:'organization',5:'pad'}\n",
    "\n",
    "if os.path.exists('sample_data/bert_lstm_crf.pth'):model.load_state_dict(torch.load('sample_data/bert_lstm_crf.pth'))\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "  model.train()\n",
    "  losses = 0.0\n",
    "  for data in trainloader:\n",
    "      tokens_tensors,label_tensors,masks_tensors = [t.cuda() for t in data]\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(input_ids = tokens_tensors,attention_mask = masks_tensors,labels = label_tensors)\n",
    "      loss = outputs[0]\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      losses += loss.item()\n",
    "  avg_train_loss = losses / len(trainloader)\n",
    "  print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "  \n",
    "  model.eval()\n",
    "  predictions , true_labels = [], []\n",
    "\n",
    "\n",
    "  if not need_CRF:\n",
    "    for data in devloader:\n",
    "      tokens_tensors, label_tensors, masks_tensors = [t.cuda() for t in data]\n",
    "      with torch.no_grad():\n",
    "        preds = model(input_ids=tokens_tensors, attention_mask=masks_tensors)\n",
    "      \n",
    "      for pred,label_tensor in zip(preds[0],label_tensors):\n",
    "        logit = pred.detach().cpu().numpy()#detach的方法，将variable参数从网络中隔离开，不参与参数更新\n",
    "        label_ids = label_tensor.cpu().numpy()\n",
    "\n",
    "        predictions.extend(np.argmax(logit, axis=1))\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    pred_tags = list(np.array(predictions).flatten())\n",
    "    valid_tags = list(np.array(true_labels).flatten())\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags,valid_tags,average='weighted')))#传入的是具体的tag\n",
    "\n",
    "  else:\n",
    "    for batch in devloader:\n",
    "      tokens_tensors, label_tensors, masks_tensors = [t.cuda() for t in data]\n",
    "      with torch.no_grad():\n",
    "        outputs = model(input_ids=tokens_tensors, attention_mask=masks_tensors,labels=label_tensors)\n",
    "      logits = outputs[1]\n",
    "      label_ids = label_tensors.cpu().numpy()\n",
    "\n",
    "      predictions.extend(logits)\n",
    "      true_labels.extend(list(label_ids))\n",
    "\n",
    "    pred_tags = [[type_label[p_i] for p, l in zip(predictions, true_labels)\n",
    "                  for p_i, l_i in zip(p, l) if type_label[l_i] != \"pad\"]]\n",
    "    valid_tags = [[type_label[l_i] for l in true_labels\n",
    "                    for l_i in l if type_label[l_i] != \"pad\"]]\n",
    "    print(\"Validation F1-Score: {}\".format(f1(pred_tags, valid_tags)))\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'sample_data/bert_lstm_crf.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb604db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '360集团发布一个漏洞'\n",
    "need_CRF = True\n",
    "test_tokens = tokenizer.tokenize(text)\n",
    "test_ids = tokenizer.convert_tokens_to_ids(test_tokens)\n",
    "test_tokens_tensor = torch.tensor(test_ids)\n",
    "test_tokens_tensor = test_tokens_tensor\n",
    "\n",
    "test_masks_tensor = torch.zeros(test_tokens_tensor.shape, dtype=torch.long)\n",
    "test_masks_tensor = test_masks_tensor.masked_fill(test_tokens_tensor != 0, 1)\n",
    "\n",
    "if not need_CRF:\n",
    "  outputs = model(input_ids=test_tokens_tensor.unsqueeze(0).cuda(),attention_mask=test_masks_tensor.unsqueeze(0).cuda())\n",
    "  logits = outputs[0]\n",
    "  preds = []\n",
    "  for logit in logits:\n",
    "    preds.extend(np.argmax(logit.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "  inverse_dict=dict([val,key] for key,val in label_type.items())\n",
    "  preds = [inverse_dict[i] for i in preds]\n",
    "\n",
    "  print(test_tokens)\n",
    "  print(preds)\n",
    "\n",
    "else:\n",
    "  logits = model(input_ids=test_tokens_tensor.unsqueeze(0).cuda(),attention_mask=test_masks_tensor.unsqueeze(0).cuda())[0]\n",
    "\n",
    "  preds = [type_label[i] for i in logits]\n",
    "\n",
    "  print(test_tokens)\n",
    "  print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892eb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f008d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UK1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
