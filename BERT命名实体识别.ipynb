{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f13bb0c",
   "metadata": {},
   "source": [
    "- github地址：(https://github.com/cshmzin/nlp-code/blob/main/bert%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.ipynb)\n",
    "- 提醒一下：这个代码在我电脑的VsCode上面运行不出来，卡在了训练的部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef87b2",
   "metadata": {},
   "source": [
    "说一下命名实体识别：\n",
    "- 数据集：json格式\n",
    "- 需要做label2id;比如说有10个实体，需要映射为0，1，2，3...【因为NER任务的本质是分类，这样做便于分类】\n",
    "    - 提示：可以将每个句子的长度len(sen),初始化为标签[0]*len(sen),然后将句子的不同实体做标签映射\n",
    "- 然后对数据集中的“text”部分，做BERT编码；\n",
    "    - 方式一：\n",
    "    ```python\n",
    "    encoding = tokenizer.encode_plus(text,.....)\n",
    "    token_ids = torch.tensor(encoding[\"token_ids\"])\n",
    "    attention_mask = torch.tensor(encoding[\"attention_mask\"])\n",
    "    token_type_ids = torch.tensor(encoding[\"token_type_ids\"])\n",
    "    return {\n",
    "        token_ids,attention_mask,token_type_ids\n",
    "    }\n",
    "    然后将token_ids,attention_mask,token_type_ids作为模型输入就行了\n",
    "```\n",
    "\n",
    "    - 方式二：\n",
    "    ```python\n",
    "    token_ids = []\n",
    "    token_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)))\n",
    "    #再通过一些手段得到attention_mask，token_type_ids\n",
    "    #然后将token_ids,attention_mask,token_type_ids作为模型输入就行了\n",
    "```\n",
    "\n",
    "- 损失函数计算：\n",
    "将预测得到的pred_id，和train_id做一个多分类交叉熵损失\n",
    "\n",
    "- 关于模型结构：\n",
    "项目中就这一行代码（可以点进去看模型结构），其实就三层【bert+dropout+Linear】,没有softmax\n",
    "\n",
    "`model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=17)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bd0ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数量：train：10748,dev：1343,test：1345\n",
      "{'text': '浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，', 'label': {'name': {'叶老桂': [[9, 11]]}, 'company': {'浙商银行': [[0, 3]]}}}\n",
      "{'text': '彭小军认为，国内银行现在走的是台湾的发卡模式，先通过跑马圈地再在圈的地里面选择客户，', 'label': {'address': {'台湾': [[15, 16]]}, 'name': {'彭小军': [[0, 2]]}}}\n",
      "{'id': 0, 'text': '四川敦煌学”。近年来，丹棱县等地一些不知名的石窟迎来了海内外的游客，他们随身携带着胡文和的著作。'}\n"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "import json\n",
    "\n",
    "train_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "\n",
    "for line in open('train.json','r',encoding='UTF-8'):\n",
    "    train_data.append(json.loads(line))\n",
    "\n",
    "for line in open('dev.json','r',encoding='UTF-8'):\n",
    "    dev_data.append(json.loads(line))\n",
    "\n",
    "for line in open('test.json','r',encoding='UTF-8'):\n",
    "    test_data.append(json.loads(line))\n",
    "    \n",
    "print(f'数量：train：{len(train_data)},dev：{len(dev_data)},test：{len(test_data)}')\n",
    "print(train_data[0])\n",
    "print(dev_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56faccda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[[9, 11]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = train_data[0]['label']['name'].values()\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34bee49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "label_type = {'o':0,'address':1,'book':2,'company':3,'game':4,'government':5,'movie':6,'name':7,'organization':8,'position':9,'scene':10}\n",
    "def decode_label(d):\n",
    "  text_len = len(d['text'])#d必须是一个字典，d=train_data[0]、train_data[1]、train_data[2]...\n",
    "  label = [0]*text_len\n",
    "  types = d['label'].keys()\n",
    "  for t in types:\n",
    "    values = d['label'][t].values()#values是[[9, 11]]或[[0, 3]]\n",
    "    si = [v for value in values for v in value]#si = [9,11]\n",
    "    for i in si:\n",
    "      for j in range(i[0],i[1]+1):\n",
    "        label[j] = label_type[t]\n",
    "  return label\n",
    "\n",
    "print(decode_label(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，\n",
      "[3, 3, 3, 3, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "王府井书店北京市东城区王府井大街&&&号\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "前几年，香港贸易发展局数次的组织寰宇娱乐、骄阳电影、美亚电影、寰亚综艺、东方电影、\n"
     ]
    }
   ],
   "source": [
    "def transfrom_data(data,mode):\n",
    "  data_texts = [re.sub('\\d','&',d['text']) for d in data]\n",
    "  \n",
    "  if mode == 'train':\n",
    "    data_labels = []\n",
    "    for d in data:\n",
    "      data_labels.append(decode_label(d))\n",
    "    return (data_texts,data_labels)\n",
    "  \n",
    "  else:\n",
    "    return data_texts \n",
    "\n",
    "train_texts,train_labels = transfrom_data(train_data,'train')\n",
    "dev_texts,dev_labels = transfrom_data(dev_data,'train')\n",
    "test_texts = transfrom_data(train_data,'test')\n",
    "\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])\n",
    "print(dev_texts[66])\n",
    "print(dev_labels[66])\n",
    "print(test_texts[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86129672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 使用bert的tokenizer将文字转化成数字。\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定为中文\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10a70ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3851, 1555, 7213, 6121, 821, 689, 928, 6587, 6956, 1383, 5439, 3424, 1300, 1894, 1156, 794, 1369, 671, 702, 6235, 2428, 2190, 758, 6887, 7305, 3546, 6822, 6121, 749, 6237, 6438, 511, 1383, 5439, 3424, 6371, 711, 8024, 2190, 4680, 1184, 1744, 1079, 1555, 689, 7213, 6121, 5445, 6241, 8024]\n",
      "[3, 3, 3, 3, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "王府井书店北京市东城区王府井大街&&&号\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 使用bert的tokenizer将文字转化成数字。\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定为中文\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "clear_output()\n",
    "\n",
    "train_ids = []\n",
    "dev_ids = []\n",
    "test_ids = []\n",
    "for train_text in train_texts:\n",
    "  train_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_text)))\n",
    "\n",
    "for dev_text in dev_texts:\n",
    "  dev_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(dev_text)))\n",
    "\n",
    "for test_text in test_texts:\n",
    "  test_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_text)))\n",
    "print(train_ids[0])\n",
    "print(train_labels[0])\n",
    "\n",
    "print(dev_texts[66])\n",
    "print(dev_labels[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80653c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "MaxLen = 40\n",
    "class NewDataset(Dataset):\n",
    "    def __init__(self,ids,labels):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.len = len(ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tokens_tensor = torch.tensor(self.ids[item])\n",
    "        label_tensor = torch.tensor(self.labels[item])\n",
    "        return (tokens_tensor,label_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "trainset = NewDataset(train_ids,train_labels)\n",
    "devset = NewDataset(dev_ids,dev_labels)\n",
    "BATCH_SIZE = 64\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    label_tensors = [s[1] for s in samples]\n",
    "\n",
    "\n",
    "    # zero pad 到同一序列长度\n",
    "    one = [0]\n",
    "    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n",
    "    label_tensors = pad_sequence(label_tensors,batch_first=True,padding_value=0)\n",
    "\n",
    "    if len(tokens_tensors[0]) != 50:\n",
    "      tokens_tensors = torch.tensor([t + one for t in tokens_tensors.numpy().tolist()])\n",
    "    if len(label_tensors[0]) != 50: \n",
    "      label_tensors = torch.tensor([t + one for t in label_tensors.numpy().tolist()])\n",
    "    # attention masks，将 tokens_tensors 不为 zero padding 的位置设为1\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "\n",
    "    return tokens_tensors, masks_tensors, label_tensors\n",
    "\n",
    "\n",
    "trainloader = DataLoader(trainset,batch_size=BATCH_SIZE,collate_fn=create_mini_batch,drop_last=False)\n",
    "devloader = DataLoader(trainset,batch_size=BATCH_SIZE,collate_fn=create_mini_batch,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3388da31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3851, 1555, 7213, 6121,  821,  689,  928, 6587, 6956, 1383, 5439, 3424,\n",
      "        1300, 1894, 1156,  794, 1369,  671,  702, 6235, 2428, 2190,  758, 6887,\n",
      "        7305, 3546, 6822, 6121,  749, 6237, 6438,  511, 1383, 5439, 3424, 6371,\n",
      "         711, 8024, 2190, 4680, 1184, 1744, 1079, 1555,  689, 7213, 6121, 5445,\n",
      "        6241, 8024])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1])\n",
      "tensor([3, 3, 3, 3, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "for a in trainloader:\n",
    "    print(a[0][0])\n",
    "    print(a[1][0])\n",
    "    print(a[2][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e91bd1",
   "metadata": {},
   "source": [
    "- 这里使用HuggingFace中封装好的BERT（一般最后一层是Linear）\n",
    "- 通常HuggingFace中封装好的BERT:`loss = outputs[0]`, `logits = outputs[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fa095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=17)\n",
    "model.cuda()\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "Epochs = 10\n",
    "for epoch in range(Epochs):\n",
    "    losses = 0.0\n",
    "    for data in trainloader:\n",
    "        tokens_tensors, masks_tensors, label_tensors = [t.cuda() for t in data]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids = tokens_tensors,attention_mask = masks_tensors,labels = label_tensors)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "nb_eval_steps = 0\n",
    "model.eval()\n",
    "eval_loss,eval_accuracy = 0,0\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for data in devloader:\n",
    "    tokens_tensors, masks_tensors, label_tensors = [t.cuda() for t in data]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=tokens_tensors, attention_mask=masks_tensors, labels=label_tensors)\n",
    "        loss = outputs[0]\n",
    "        preds = model(input_ids=tokens_tensors, attention_mask=masks_tensors)\n",
    "\n",
    "    for pred,label_tensor in zip(preds[0],label_tensors):\n",
    "      logit = pred.detach().cpu().numpy()#detach的方法，将variable参数从网络中隔离开，不参与参数更新\n",
    "      label_ids = label_tensor.cpu().numpy()\n",
    "\n",
    "      predictions.extend(np.argmax(logit, axis=1))\n",
    "      true_labels.append(label_ids)\n",
    "      # 计算accuracy 和 loss\n",
    "      tmp_eval_accuracy = flat_accuracy(logit, label_ids)\n",
    "\n",
    "      eval_loss += loss.mean().item()\n",
    "      eval_accuracy += tmp_eval_accuracy\n",
    "      nb_eval_steps += 1\n",
    "\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pred_tags = list(np.array(predictions).flatten())\n",
    "valid_tags = list(np.array(true_labels).flatten())\n",
    "print(pred_tags[0:20])\n",
    "print(valid_tags[0:20])\n",
    "print(\"F1-Score: {}\".format(f1_score(pred_tags,valid_tags,average='weighted')))#传入的是具体的tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '普京是俄罗斯的总统'\n",
    "test_tokens = tokenizer.tokenize(text)\n",
    "test_ids = tokenizer.convert_tokens_to_ids(test_tokens)\n",
    "test_tokens_tensor = torch.tensor(test_ids)\n",
    "test_tokens_tensor = test_tokens_tensor\n",
    "\n",
    "test_masks_tensor = torch.zeros(test_tokens_tensor.shape, dtype=torch.long)\n",
    "test_masks_tensor = test_masks_tensor.masked_fill(test_tokens_tensor != 0, 1)\n",
    "\n",
    "outputs = model(input_ids=test_tokens_tensor.unsqueeze(0).cuda(),attention_mask=test_masks_tensor.unsqueeze(0).cuda())\n",
    "logits = outputs[0]\n",
    "preds = []\n",
    "for logit in logits:\n",
    "  preds.extend(np.argmax(logit.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "inverse_dict=dict([val,key] for key,val in label_type.items())\n",
    "preds = [inverse_dict[i] for i in preds]\n",
    "\n",
    "print(test_tokens)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba7198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#开个小差；判断a是不是一个list\n",
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4bdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UK1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
